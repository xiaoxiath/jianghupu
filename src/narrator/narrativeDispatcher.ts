/**
 * @file Narrative Dispatcher
 * @description æ ¹æ®ä¸Šä¸‹æ–‡å’Œè§„åˆ™ï¼Œå°†å™äº‹ç”Ÿæˆè¯·æ±‚åˆ†å‘åˆ°ä¸åŒçš„å±‚çº§ï¼ˆæ¨¡æ¿ã€è½»é‡æ¨¡å‹ã€é‡é‡çº§æ¨¡å‹ï¼‰ã€‚
 */
import { singleton, inject } from 'tsyringe';
import { AICoreService } from '../core/ai/AICoreService';
import { PromptManager } from './promptManager';
import { TemplateEngine } from './templateEngine';
import { CostMonitorService } from '../core/monitoring/costMonitorService';
import aiConfig from '../config/ai';
import * as fs from 'fs';
import * as path from 'path';
import type { BardPrompt, BardOutput, RawLLMOption, TradeInfo, SkillMasterInfo, ItemIdentificationInfo } from './aiBard';
import type { EventChoice } from '../core/events/types';

// å®šä¹‰å™äº‹å±‚çº§
export enum NarrativeLayer {
  L0_Template = 'L0_Template', // æ¨¡æ¿å±‚ï¼Œç”¨äºé«˜é¢‘ã€ç®€å•äº‹ä»¶
  L1_LightLLM = 'L1_LightLLM', // è½»é‡çº§æ¨¡å‹ï¼Œç”¨äºä¸­ç­‰å¤æ‚åº¦çš„å™äº‹
  L2_HeavyLLM = 'L2_HeavyLLM', // é‡é‡çº§æ¨¡å‹ï¼Œç”¨äºå…³é”®å‰§æƒ…å’Œå¤æ‚åœºæ™¯
}

// å®šä¹‰åˆ†å‘ä¸Šä¸‹æ–‡
export interface DispatchContext extends BardPrompt {
  // æœªæ¥å¯ä»¥æ‰©å±•æ›´å¤šç”¨äºå†³ç­–çš„å­—æ®µï¼Œå¦‚ eventType, importance ç­‰
  eventType?: string;
  importance?: number;
}

interface NarrativeRule {
  comment?: string;
  conditions: Partial<DispatchContext>;
  layer: NarrativeLayer;
  model?: string;
}

const STYLE_INSTRUCTIONS = {
  'å®¿å‘½': 'ä½ çš„è¯­è¨€é£æ ¼è‹å‡‰ã€åšé‡ï¼Œå¤šç”¨â€œç»ˆå°†â€ã€â€œåŠ«æ•°â€ã€â€œå¤©æ„å¦‚æ­¤â€ç­‰è¯è¯­ï¼Œå¼ºè°ƒå› æœå¾ªç¯å’Œå‘½è¿çš„ä¸å¯æŠ—æ‹’ã€‚',
  'è¯™è°': 'ä½ çš„è¯­è¨€é£æ ¼è½»æ¾ã€å¹½é»˜ï¼Œç•¥å¸¦è°ƒä¾ƒï¼Œå¸¸ç”¨â€œä¸æ–™â€ã€â€œåç”Ÿâ€ã€â€œç«Ÿâ€ç­‰è¯è¯­ï¼Œå–„äºå‘ç°æƒ…å¢ƒä¸­æœ‰è¶£æˆ–çŸ›ç›¾çš„ä¸€é¢ã€‚',
  'å“²ç†': 'ä½ çš„è¯­è¨€é£æ ¼å¼•äººæ·±æ€ï¼Œå–œæ¬¢æ¢è®¨äººå¿ƒã€å–„æ¶ã€ä¾ ä¹‹å®šä¹‰ï¼Œå¸¸ç”¨â€œä½•ä¸º...â€ã€â€œé“ä¸é­”â€ã€â€œä¸€å¿µä¹‹é—´â€ç­‰å¥å¼ã€‚',
  'ç–¯ç™«': 'ä½ çš„è¯­è¨€é£æ ¼æ··ä¹±ã€æ— åºï¼Œå……æ»¡å‘“è¯­å’Œä¸è¿è´¯çš„ç‰‡æ®µï¼Œå¸¸å¸¸å¤¹æ‚ç€â€œå˜¿å˜¿â€ã€â€œè¡€â€ã€â€œæ€â€ç­‰è¯è¯­ï¼Œä»¤äººä¸å¯’è€Œæ —ã€‚'
};

@singleton()
export class NarrativeDispatcher {
  private rules: NarrativeRule[] = [];

  constructor(
    @inject(AICoreService) private aiService: AICoreService,
    @inject(PromptManager) private promptManager: PromptManager,
    @inject(TemplateEngine) private templateEngine: TemplateEngine,
    @inject(CostMonitorService) private costMonitor: CostMonitorService
  ) {
    this.loadRules();
  }

  private loadRules() {
    try {
      // Correctly resolve path from the project root
      const rulesPath = path.resolve(process.cwd(), 'src', 'config', 'narrative_rules.json');
      const rulesFile = fs.readFileSync(rulesPath, 'utf-8');
      this.rules = JSON.parse(rulesFile).rules;
      console.log(`[NarrativeDispatcher] Loaded ${this.rules.length} narrative rules.`);
    } catch (error) {
      console.error('[NarrativeDispatcher] Failed to load narrative rules:', error);
      // åœ¨æ²¡æœ‰è§„åˆ™çš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿå°†å›é€€åˆ°é»˜è®¤è¡Œä¸º
      this.rules = [];
    }
  }

  /**
   * æ ¹æ®ä¸Šä¸‹æ–‡å†³å®šä½¿ç”¨å“ªä¸ªå™äº‹å±‚çº§ã€‚
   * @param context - The dispatch context.
   * @returns The determined narrative layer.
   */
  private determineLayer(context: DispatchContext): { layer: NarrativeLayer; model?: string } {
    for (const rule of this.rules) {
      const conditions = rule.conditions;
      const isMatch = (Object.keys(conditions) as Array<keyof typeof conditions>).every(key => {
        return conditions[key] === context[key];
      });

      if (isMatch) {
        console.log(`[NarrativeDispatcher] Matched rule: ${rule.comment || 'Untitled Rule'}`);
        return { layer: rule.layer, model: rule.model };
      }
    }

    // é»˜è®¤è¿”å›é‡é‡çº§æ¨¡å‹å±‚
    return { layer: NarrativeLayer.L2_HeavyLLM };
  }

  /**
   * å°†å™äº‹è¯·æ±‚åˆ†å‘åˆ°é€‚å½“çš„å±‚çº§ã€‚
   * @param context - The full context for generating a narrative.
   * @returns A promise that resolves to the generated BardOutput.
   */
  public async dispatch(context: DispatchContext): Promise<BardOutput> {
    const { layer, model } = this.determineLayer(context);

    switch (layer) {
      case NarrativeLayer.L0_Template:
        console.log('Dispatching to L0 Template Engine...');
        const templateOutput = await this.templateEngine.render(`${context.eventType}.njk`, context);
        if (templateOutput) {
          return templateOutput;
        }
        console.warn(`Template rendering failed for "${context.eventType}". Falling back to L2.`);
        return this.generateWithHeavyLLM(context);
      case NarrativeLayer.L1_LightLLM:
        console.log(`Dispatching to L1 Light LLM (Model: ${model})...`);
        if (!model) {
          console.error('L1 LightLLM rule is missing a "model" definition. Falling back to L2.');
          return this.generateWithHeavyLLM(context);
        }
        return this.generateWithLightLLM(context, model);
      case NarrativeLayer.L2_HeavyLLM:
        console.log('Dispatching to L2 Heavy LLM...');
        return this.generateWithHeavyLLM(context);
      default:
        throw new Error(`Unknown narrative layer: ${layer}`);
    }
  }

  public async generateRaw(prompt: string): Promise<{ success: boolean; content: string | null; error?: string }> {
    const response = await this.aiService.generate({
      prompt: prompt,
      format: 'json',
    });
    if (!response.success || !response.content) {
      return { success: false, content: null, error: response.error };
    }
    return { success: true, content: response.content };
  }

  public async generateTradeScene(playerState: any, worldContext: any): Promise<TradeInfo | null> {
    const templateData = {
      player: playerState,
      location_name: worldContext.location.name,
    };
    const fullPrompt = this.promptManager.buildPrompt('trader', templateData);
    const response = await this.generateRaw(fullPrompt);
    return this._parseJsonResponse<TradeInfo>(response, 'Trader AI');
  }

  public async generateSkillMasterScene(playerState: any): Promise<SkillMasterInfo | null> {
    const templateData = { player: playerState };
    const fullPrompt = this.promptManager.buildPrompt('skill_master', templateData);
    const response = await this.generateRaw(fullPrompt);
    return this._parseJsonResponse<SkillMasterInfo>(response, 'Skill Master AI');
  }

  public async identifyItem(item: any): Promise<ItemIdentificationInfo | null> {
    const templateData = { item_to_identify: item };
    const fullPrompt = this.promptManager.buildPrompt('item_master', templateData);
    const response = await this.generateRaw(fullPrompt);
    return this._parseJsonResponse<ItemIdentificationInfo>(response, 'Item Master AI');
  }

  public async generateNpcGrowthNarrative(npc: any, oldStrength: number, newStrength: number): Promise<string | null> {
    const templateData = {
      npc,
      old_strength: oldStrength,
      new_strength: newStrength,
    };
    const fullPrompt = this.promptManager.buildPrompt('npc_growth', templateData);
    const response = await this.aiService.generate({
      prompt: fullPrompt,
    });

    if (!response.success || !response.content) {
      console.error('NPC Growth AI failed to respond.');
      return null;
    }

    return response.content;
  }

  private async generateWithLightLLM(promptData: DispatchContext, model: string): Promise<BardOutput> {
    const templateData = {
      ...promptData.playerState,
      ...promptData.worldContext,
      ...promptData,
      location_name: promptData.worldContext.location.name,
      location_description: promptData.worldContext.location.description,
      world_summary: promptData.worldContext.summary,
      legacy_summary: promptData.legacySummary || 'æ— ',
      faction_context: promptData.factionContext || 'æ— ',
      style_instruction: STYLE_INSTRUCTIONS[promptData.tone] || '',
    };

    const fullPrompt = this.promptManager.buildPrompt('narrator', templateData);
    console.log(`ğŸ¤– AI Bard (Light) is thinking... Model: ${model}, Style: ${promptData.tone}`);

    const response = await this.aiService.generate({
      prompt: fullPrompt,
      format: 'json',
      model: model, // ä¼ é€’æ¨¡å‹åç§°
    });

    this.costMonitor.recordCall({
      layer: NarrativeLayer.L1_LightLLM,
      model,
      ...response.metadata,
    });

    return this._parseAndProcessLLMResponse(response);
  }

  private async generateWithHeavyLLM(promptData: DispatchContext): Promise<BardOutput> {
    const templateData = {
      ...promptData.playerState,
      ...promptData.worldContext,
      ...promptData,
      location_name: promptData.worldContext.location.name,
      location_description: promptData.worldContext.location.description,
      world_summary: promptData.worldContext.summary,
      legacy_summary: promptData.legacySummary || 'æ— ',
      faction_context: promptData.factionContext || 'æ— ',
      style_instruction: STYLE_INSTRUCTIONS[promptData.tone] || '',
    };
    
    const fullPrompt = this.promptManager.buildPrompt('narrator', templateData);
    console.log('ğŸ¤– AI Bard (Heavy) is thinking... Style: ' + promptData.tone);

    const response = await this.aiService.generate({
      prompt: fullPrompt,
      format: 'json',
      // No model specified, so it will use the default heavy model from config
    });

    this.costMonitor.recordCall({
      layer: NarrativeLayer.L2_HeavyLLM,
      model: aiConfig.config.model, // ä»é…ç½®ä¸­è·å–é»˜è®¤çš„é‡é‡çº§æ¨¡å‹åç§°
      ...response.metadata,
    });

    return this._parseAndProcessLLMResponse(response);
  }

  private _parseAndProcessLLMResponse(response: { success: boolean; content: string | null; error?: string }): BardOutput {
    if (!response.success || !response.content) {
      console.error('Error calling AI Core Service:', response.error);
      return {
        narration: 'ï¼ˆAIè¯´ä¹¦äººæš‚æ—¶èµ°ç¥äº†ï¼Œä¸€è‚¡ç¥ç§˜çš„åŠ›é‡è®©ä½ çœ‹åˆ°äº†ä¸–ç•Œçš„çœŸå®é¢è²Œã€‚ï¼‰',
        options: [
          { text: '1. [è°ƒè¯•] æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ', action: 'debug' },
          { text: '2. [è°ƒè¯•] æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯æ—¥å¿—', action: 'debug' },
          { text: '3. [è°ƒè¯•] å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨¡å‹', action: 'debug' },
        ],
      };
    }

    const parsedContent = this._parseJsonResponse<{ narration: string; options: RawLLMOption[] }>(response, 'Narrator AI');

    if (!parsedContent) {
      return {
        narration: 'ï¼ˆAIè¯´ä¹¦äººè¨€è¯­é”™ä¹±ï¼Œä¼¼ä¹çœ‹åˆ°äº†æ— æ³•ç†è§£çš„æ™¯è±¡ã€‚ï¼‰',
        options: [
          { text: '1. [è°ƒè¯•] æ£€æŸ¥è¿”å›çš„ JSON ç»“æ„æ˜¯å¦æ­£ç¡®', action: 'debug' },
          { text: '2. [è°ƒè¯•] æŸ¥çœ‹ narrativeDispatcher.ts ä¸­çš„è§£æé€»è¾‘', action: 'debug' },
        ],
      };
    }

    try {
      if (typeof parsedContent.narration !== 'string') {
        throw new Error(`LLM response is missing narration: ${JSON.stringify(parsedContent)}`);
      }

      let finalOptions: EventChoice[];

      if (Array.isArray(parsedContent.options) && parsedContent.options.length > 0) {
        finalOptions = parsedContent.options
          .map((opt: RawLLMOption) => ({
            text: (typeof opt.text === 'string' ? opt.text.replace(/^\d+\.\s*/, '').trim() : ''),
            action: 'narrate',
            result: opt.result || { description: `ä½ é€‰æ‹©äº†"${opt.text}"` },
          }))
          .filter(opt => opt.text.length > 0);
      } else {
        finalOptions = [];
      }

      if (finalOptions.length === 0) {
        console.warn(`LLM returned empty or invalid options. Providing a default option. Raw options: ${JSON.stringify(parsedContent.options)}`);
        finalOptions = [{
          text: 'ç»§ç»­...',
          action: 'narrate',
          result: { description: 'ä½ å†³å®šç»§ç»­å‰è¡Œã€‚' }
        }];
      }

      const finalOutput: BardOutput = {
          narration: parsedContent.narration,
          options: finalOptions,
      };

      return finalOutput;

    } catch (error) {
      console.error('Error processing AI response in dispatcher:', error);
      return {
        narration: 'ï¼ˆAIè¯´ä¹¦äººè¨€è¯­é”™ä¹±ï¼Œä¼¼ä¹çœ‹åˆ°äº†æ— æ³•ç†è§£çš„æ™¯è±¡ã€‚ï¼‰',
        options: [
          { text: '1. [è°ƒè¯•] æ£€æŸ¥è¿”å›çš„ JSON ç»“æ„æ˜¯å¦æ­£ç¡®', action: 'debug' },
          { text: '2. [è°ƒè¯•] æŸ¥çœ‹ narrativeDispatcher.ts ä¸­çš„è§£æé€»è¾‘', action: 'debug' },
        ],
      };
    }
  }

  private _parseJsonResponse<T>(
    response: { success: boolean; content: string | null; error?: string },
    aiName: string = 'AI'
  ): T | null {
    if (!response.success || !response.content) {
      console.error(`${aiName} failed to respond:`, response.error);
      return null;
    }

    try {
      let jsonString = response.content;
      const jsonMatch = jsonString.match(/```json\s*([\s\S]*?)\s*```|({[\s\S]*})/);
      const potentialJson = jsonMatch ? jsonMatch[1] || jsonMatch[2] : null;

      if (potentialJson) {
        jsonString = potentialJson;
      } else {
        const startIndex = jsonString.indexOf('{');
        const endIndex = jsonString.lastIndexOf('}');
        if (startIndex !== -1 && endIndex !== -1 && endIndex > startIndex) {
          jsonString = jsonString.substring(startIndex, endIndex + 1);
        } else {
          throw new Error(`Could not find a valid JSON object in the response: ${response.content}`);
        }
      }
      
      return JSON.parse(jsonString) as T;
    } catch (error) {
      console.error(`Failed to parse ${aiName} response:`, error, "Raw response:", response.content);
      return null;
    }
  }
}